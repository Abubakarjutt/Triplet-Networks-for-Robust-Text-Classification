{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>is_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>This: :One can make an analogy in mathematical...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>`  :Clarification for you  (and Zundark's righ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>`This is such a fun entry.   Devotchka  I once...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37330.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>`   I fixed the link; I also removed ``homeopa...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37346.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>`If they are ``indisputable`` then why does th...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95687</th>\n",
       "      <td>699822249.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>`  :``Comment````. Gentlemen, this article pro...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95688</th>\n",
       "      <td>699826615.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>*Support and recommend moving this (and my re...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95689</th>\n",
       "      <td>699843603.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>`  == File:Romantic Warriors cover.jpg ==  You...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95690</th>\n",
       "      <td>699848324.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>`   These sources don't exactly exude a sense ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95691</th>\n",
       "      <td>699891012.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>== Warning ==  There is clearly a protection...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95692 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            rev_id  toxicity  \\\n",
       "0           2232.0       0.1   \n",
       "1           4216.0       0.0   \n",
       "2          26547.0       0.0   \n",
       "3          37330.0       0.3   \n",
       "4          37346.0       0.1   \n",
       "...            ...       ...   \n",
       "95687  699822249.0       0.2   \n",
       "95688  699826615.0       0.0   \n",
       "95689  699843603.0       0.0   \n",
       "95690  699848324.0       0.0   \n",
       "95691  699891012.0       0.4   \n",
       "\n",
       "                                                 comment  year  logged_in  \\\n",
       "0      This: :One can make an analogy in mathematical...  2002       True   \n",
       "1      `  :Clarification for you  (and Zundark's righ...  2002       True   \n",
       "2      `This is such a fun entry.   Devotchka  I once...  2002       True   \n",
       "3      `   I fixed the link; I also removed ``homeopa...  2002       True   \n",
       "4      `If they are ``indisputable`` then why does th...  2002       True   \n",
       "...                                                  ...   ...        ...   \n",
       "95687  `  :``Comment````. Gentlemen, this article pro...  2016       True   \n",
       "95688   *Support and recommend moving this (and my re...  2016       True   \n",
       "95689  `  == File:Romantic Warriors cover.jpg ==  You...  2016       True   \n",
       "95690  `   These sources don't exactly exude a sense ...  2016       True   \n",
       "95691    == Warning ==  There is clearly a protection...  2016       True   \n",
       "\n",
       "            ns   sample  split  is_toxic  \n",
       "0      article   random  train     False  \n",
       "1         user   random  train     False  \n",
       "2      article   random  train     False  \n",
       "3      article   random  train     False  \n",
       "4      article   random  train     False  \n",
       "...        ...      ...    ...       ...  \n",
       "95687  article  blocked  train     False  \n",
       "95688  article   random  train     False  \n",
       "95689     user   random  train     False  \n",
       "95690  article  blocked  train     False  \n",
       "95691     user  blocked  train     False  \n",
       "\n",
       "[95692 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/wiki_train.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = \" \".join(re.findall(\"[a-zA-Z]+\", str(text)))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(dataframe, column):\n",
    "    tokens = []\n",
    "    for i in tqdm_notebook(dataframe[column][:]):\n",
    "        _tokens = word_tokenize(str(preprocess(i)))\n",
    "        tokens.append(_tokens)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abubakar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d17391f77c4518ac263447b56cef12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=95692.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98d5d4af8434629828131cae61864db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630d7d56b474498c973a4c7eb3228664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76564.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abubakar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d083897c50fa4a599161722a9ca61b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76564.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/wiki_train.csv')\n",
    "train_data = train_data.dropna(axis = 0)\n",
    "\n",
    "val_data = pd.read_csv('data/wiki_dev.csv')\n",
    "val_data = val_data.dropna(axis = 0)\n",
    "\n",
    "df_test = pd.read_csv('test_data.csv')\n",
    "df_test = df_test.dropna(axis = 0)\n",
    "df_test.loc[df_test['Label'] == 'BAD', 'Label'] = 1\n",
    "df_test.loc[df_test['Label'] == 'NOT_BAD', 'Label'] = 0\n",
    "\n",
    "\n",
    "train_feature = get_tokens(train_data, 'comment')\n",
    "train_label = train_data['toxicity']\n",
    "\n",
    "\n",
    "\n",
    "val_feature = get_tokens(val_data, 'comment')\n",
    "val_label = val_data['toxicity']\n",
    "\n",
    "test_feature = get_tokens(df_test, 'Text')\n",
    "test_label = df_test['Label']\n",
    "\n",
    "identity_terms = []\n",
    "for i in tqdm_notebook(range(len(df_test['Text']))):\n",
    "    _comment = df_test.loc[i,  'Text'].split(\" \")\n",
    "    if len(_comment) < 3:\n",
    "        _term = _comment[1]\n",
    "        identity_terms.append(_term)\n",
    "identity_terms = list(set(identity_terms))\n",
    "\n",
    "\n",
    "terms = []\n",
    "for i in range(len(df_test['Text'])):\n",
    "    _text = df_test.loc[i, 'Text'].split(' ')\n",
    "    _term = list(set(_text).intersection(set(identity_terms)))\n",
    "    if len(_term) > 0:\n",
    "        terms.append(_term[0])\n",
    "    else:\n",
    "        terms.append(np.nan)\n",
    "        \n",
    "df_test['Identity_Terms'] = terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_feature)\n",
    "def vectorize(sent):\n",
    "    vector = tokenizer.texts_to_sequences([sent])\n",
    "    vector= pad_sequences(vector, maxlen = 50, dtype=\"int32\")\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletText(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
    "    Test: Creates fixed triplets for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.text = dataset.comment\n",
    "        self.labels = dataset.toxicity.round()\n",
    "\n",
    "        self.train_labels = self.labels\n",
    "        self.train_data = self.text\n",
    "        self.labels_set = set(self.labels.round())\n",
    "        self.label_to_indices = {label: np.where(self.labels.round() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text1, label1 = self.text[index], self.labels[index].round().item()\n",
    "        positive_index = index\n",
    "        while positive_index == index:\n",
    "            positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "        negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "        negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
    "        text2 = self.train_data[positive_index]\n",
    "        text3 = self.train_data[negative_index]\n",
    "\n",
    "        return (text1, text2, text3), []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_train_dataset = TripletText(train_data.loc[:, :])\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "triplet_val_dataset = TripletText(val_data.loc[:, :])\n",
    "triplet_val_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(50, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 512),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(512, 256)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    \n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        output3 = self.embedding_net(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(0)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(0)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, log_interval, metrics=[],\n",
    "        start_epoch=0):\n",
    "    \"\"\"\n",
    "    Loaders, model, loss function and metrics should work together for a given task,\n",
    "    i.e. The model should be able to process data output of loaders,\n",
    "    loss function should process target output of loaders and outputs from the model\n",
    "    Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n",
    "    Siamese network: Siamese loader, siamese model, contrastive loss\n",
    "    Online triplet learning: batch loader, embedding model, online triplet loss\n",
    "    \"\"\"\n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        scheduler.step()\n",
    "\n",
    "        # Train stage\n",
    "        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, log_interval, metrics)\n",
    "\n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "        val_loss, metrics = test_epoch(val_loader, model, loss_fn, metrics)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n",
    "                                                                                 val_loss)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "        print(message)\n",
    "        print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "def train_epoch(train_loader, model, loss_fn, optimizer, log_interval, metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        text1 = torch.FloatTensor(vectorize(preprocess(list(data[0])))[0])\n",
    "        text2 = torch.FloatTensor(vectorize(preprocess(list(data[1])))[0])\n",
    "        text3 = torch.FloatTensor(vectorize(preprocess(list(data[2])))[0])\n",
    "        target = target if len(target) > 0 else None\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text1, text2, text3)\n",
    "\n",
    "        if type(outputs) not in (tuple, list):\n",
    "            outputs = (outputs,)\n",
    "\n",
    "        loss_inputs = outputs\n",
    "        if target is not None:\n",
    "            target = (target,)\n",
    "            loss_inputs += target\n",
    "\n",
    "        loss_outputs = loss_fn(*loss_inputs)\n",
    "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric(outputs, target, loss_outputs)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "            print(message)\n",
    "            losses = []\n",
    "\n",
    "    total_loss /= (batch_idx + 1)\n",
    "    return total_loss, metrics\n",
    "\n",
    "def test_epoch(val_loader, model, loss_fn, metrics):\n",
    "    with torch.no_grad():\n",
    "        for metric in metrics:\n",
    "            metric.reset()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            text1 = torch.FloatTensor(vectorize(preprocess(list(data[0])))[0])\n",
    "            text2 = torch.FloatTensor(vectorize(preprocess(list(data[1])))[0])\n",
    "            text3 = torch.FloatTensor(vectorize(preprocess(list(data[2])))[0])\n",
    "\n",
    "            outputs = model(text1, text2, text3)\n",
    "\n",
    "            if type(outputs) not in (tuple, list):\n",
    "                outputs = (outputs,)\n",
    "            loss_inputs = outputs\n",
    "            if target is not None:\n",
    "                target = (target,)\n",
    "                loss_inputs += target\n",
    "\n",
    "            loss_outputs = loss_fn(*loss_inputs)\n",
    "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric(outputs, target, loss_outputs)\n",
    "\n",
    "    return val_loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.3\n",
    "embedding_net = EmbeddingNet()\n",
    "model = TripletNet(embedding_net)\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 0.00001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abubakar\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/95692 (0%)]\tLoss: 9534.543945\n",
      "Train: [16000/95692 (17%)]\tLoss: 111719.685031\n",
      "Train: [32000/95692 (33%)]\tLoss: 35882.349026\n",
      "Train: [48000/95692 (50%)]\tLoss: 53986.397664\n",
      "Train: [64000/95692 (67%)]\tLoss: 32093.430373\n",
      "Train: [80000/95692 (84%)]\tLoss: 9264.413528\n",
      "Epoch: 1/20. Train set: Average loss: 47212.7443\n",
      "Epoch: 1/20. Validation set: Average loss: 27599.8315\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 1043.746338\n",
      "Train: [16000/95692 (17%)]\tLoss: 20477.236874\n",
      "Train: [32000/95692 (33%)]\tLoss: 9624.985048\n",
      "Train: [48000/95692 (50%)]\tLoss: 27908.465734\n",
      "Train: [64000/95692 (67%)]\tLoss: 13083.507478\n",
      "Train: [80000/95692 (84%)]\tLoss: 5089.658337\n",
      "Epoch: 2/20. Train set: Average loss: 17337.5426\n",
      "Epoch: 2/20. Validation set: Average loss: 8625.7889\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 6578.131836\n",
      "Train: [16000/95692 (17%)]\tLoss: 12060.476505\n",
      "Train: [32000/95692 (33%)]\tLoss: 16236.831876\n",
      "Train: [48000/95692 (50%)]\tLoss: 7870.752872\n",
      "Train: [64000/95692 (67%)]\tLoss: 12595.991835\n",
      "Train: [80000/95692 (84%)]\tLoss: 8221.438159\n",
      "Epoch: 3/20. Train set: Average loss: 10844.9372\n",
      "Epoch: 3/20. Validation set: Average loss: 7466.8436\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 22146.585938\n",
      "Train: [16000/95692 (17%)]\tLoss: 15088.094996\n",
      "Train: [32000/95692 (33%)]\tLoss: 10116.046262\n",
      "Train: [48000/95692 (50%)]\tLoss: 8273.521071\n",
      "Train: [64000/95692 (67%)]\tLoss: 5755.781891\n",
      "Train: [80000/95692 (84%)]\tLoss: 4411.844155\n",
      "Epoch: 4/20. Train set: Average loss: 7751.9823\n",
      "Epoch: 4/20. Validation set: Average loss: 6220.1928\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n",
      "Train: [16000/95692 (17%)]\tLoss: 9192.966770\n",
      "Train: [32000/95692 (33%)]\tLoss: 7845.496701\n",
      "Train: [48000/95692 (50%)]\tLoss: 2570.947613\n",
      "Train: [64000/95692 (67%)]\tLoss: 10297.772880\n",
      "Train: [80000/95692 (84%)]\tLoss: 4547.042635\n",
      "Epoch: 5/20. Train set: Average loss: 8197.6968\n",
      "Epoch: 5/20. Validation set: Average loss: 3804.9762\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 1073.334839\n",
      "Train: [16000/95692 (17%)]\tLoss: 5778.305930\n",
      "Train: [32000/95692 (33%)]\tLoss: 3113.650841\n",
      "Train: [48000/95692 (50%)]\tLoss: 3928.120933\n",
      "Train: [64000/95692 (67%)]\tLoss: 4006.870875\n",
      "Train: [80000/95692 (84%)]\tLoss: 7009.744066\n",
      "Epoch: 6/20. Train set: Average loss: 4310.6793\n",
      "Epoch: 6/20. Validation set: Average loss: 3471.1091\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 108.693680\n",
      "Train: [16000/95692 (17%)]\tLoss: 1687.014657\n",
      "Train: [32000/95692 (33%)]\tLoss: 6848.367924\n",
      "Train: [48000/95692 (50%)]\tLoss: 2964.169030\n",
      "Train: [64000/95692 (67%)]\tLoss: 1924.399259\n",
      "Train: [80000/95692 (84%)]\tLoss: 2615.486564\n",
      "Epoch: 7/20. Train set: Average loss: 2996.3869\n",
      "Epoch: 7/20. Validation set: Average loss: 2597.8419\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n",
      "Train: [16000/95692 (17%)]\tLoss: 2523.087151\n",
      "Train: [32000/95692 (33%)]\tLoss: 3050.603023\n",
      "Train: [48000/95692 (50%)]\tLoss: 18330.822367\n",
      "Train: [64000/95692 (67%)]\tLoss: 1958.909150\n",
      "Train: [80000/95692 (84%)]\tLoss: 3581.278252\n",
      "Epoch: 8/20. Train set: Average loss: 5104.3454\n",
      "Epoch: 8/20. Validation set: Average loss: 4393.3042\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n",
      "Train: [16000/95692 (17%)]\tLoss: 7248.408801\n",
      "Train: [32000/95692 (33%)]\tLoss: 2466.241632\n",
      "Train: [48000/95692 (50%)]\tLoss: 2061.053650\n",
      "Train: [64000/95692 (67%)]\tLoss: 6895.408966\n",
      "Train: [80000/95692 (84%)]\tLoss: 1129.160802\n",
      "Epoch: 9/20. Train set: Average loss: 3778.8980\n",
      "Epoch: 9/20. Validation set: Average loss: 1862.6259\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 92.074173\n",
      "Train: [16000/95692 (17%)]\tLoss: 2528.001982\n",
      "Train: [32000/95692 (33%)]\tLoss: 1737.321330\n",
      "Train: [48000/95692 (50%)]\tLoss: 1465.402542\n",
      "Train: [64000/95692 (67%)]\tLoss: 1318.214241\n",
      "Train: [80000/95692 (84%)]\tLoss: 1902.410751\n",
      "Epoch: 10/20. Train set: Average loss: 1871.8249\n",
      "Epoch: 10/20. Validation set: Average loss: 2403.4898\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 378.319275\n",
      "Train: [16000/95692 (17%)]\tLoss: 1372.046231\n",
      "Train: [32000/95692 (33%)]\tLoss: 1611.971405\n",
      "Train: [48000/95692 (50%)]\tLoss: 1834.365693\n",
      "Train: [64000/95692 (67%)]\tLoss: 4862.126611\n",
      "Train: [80000/95692 (84%)]\tLoss: 2050.066634\n",
      "Epoch: 11/20. Train set: Average loss: 3335.9117\n",
      "Epoch: 11/20. Validation set: Average loss: 2334.5625\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n",
      "Train: [16000/95692 (17%)]\tLoss: 3175.296890\n",
      "Train: [32000/95692 (33%)]\tLoss: 6684.892691\n",
      "Train: [48000/95692 (50%)]\tLoss: 4463.444528\n",
      "Train: [64000/95692 (67%)]\tLoss: 5020.829355\n",
      "Train: [80000/95692 (84%)]\tLoss: 1163.931011\n",
      "Epoch: 12/20. Train set: Average loss: 3658.7341\n",
      "Epoch: 12/20. Validation set: Average loss: 2090.6468\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n",
      "Train: [16000/95692 (17%)]\tLoss: 29242.936400\n",
      "Train: [32000/95692 (33%)]\tLoss: 1364.118160\n",
      "Train: [48000/95692 (50%)]\tLoss: 1579.282468\n",
      "Train: [64000/95692 (67%)]\tLoss: 1232.771833\n",
      "Train: [80000/95692 (84%)]\tLoss: 873.729301\n",
      "Epoch: 13/20. Train set: Average loss: 5963.3978\n",
      "Epoch: 13/20. Validation set: Average loss: 2944.2289\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n",
      "Train: [16000/95692 (17%)]\tLoss: 5566.849994\n",
      "Train: [32000/95692 (33%)]\tLoss: 1737.947542\n",
      "Train: [48000/95692 (50%)]\tLoss: 24836.214349\n",
      "Train: [64000/95692 (67%)]\tLoss: 1350.862451\n",
      "Train: [80000/95692 (84%)]\tLoss: 990.309699\n",
      "Epoch: 14/20. Train set: Average loss: 5902.0176\n",
      "Epoch: 14/20. Validation set: Average loss: 2394.7884\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n",
      "Train: [16000/95692 (17%)]\tLoss: 1955.350496\n",
      "Train: [32000/95692 (33%)]\tLoss: 8429.445016\n",
      "Train: [48000/95692 (50%)]\tLoss: 1327.468310\n",
      "Train: [64000/95692 (67%)]\tLoss: 5257.181309\n",
      "Train: [80000/95692 (84%)]\tLoss: 1986.029714\n",
      "Epoch: 15/20. Train set: Average loss: 3401.6029\n",
      "Epoch: 15/20. Validation set: Average loss: 1861.6083\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 520.709045\n",
      "Train: [16000/95692 (17%)]\tLoss: 8006.453373\n",
      "Train: [32000/95692 (33%)]\tLoss: 3080.869411\n",
      "Train: [48000/95692 (50%)]\tLoss: 757.752586\n",
      "Train: [64000/95692 (67%)]\tLoss: 945.191756\n",
      "Train: [80000/95692 (84%)]\tLoss: 1521.049926\n",
      "Epoch: 16/20. Train set: Average loss: 2609.6322\n",
      "Epoch: 16/20. Validation set: Average loss: 1676.8969\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 33.044689\n",
      "Train: [16000/95692 (17%)]\tLoss: 1170.891416\n",
      "Train: [32000/95692 (33%)]\tLoss: 1816.361207\n",
      "Train: [48000/95692 (50%)]\tLoss: 3118.104748\n",
      "Train: [64000/95692 (67%)]\tLoss: 1449.528310\n",
      "Train: [80000/95692 (84%)]\tLoss: 1519.955271\n",
      "Epoch: 17/20. Train set: Average loss: 2006.4521\n",
      "Epoch: 17/20. Validation set: Average loss: 1679.1947\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 893.214966\n",
      "Train: [16000/95692 (17%)]\tLoss: 1929.738938\n",
      "Train: [32000/95692 (33%)]\tLoss: 3172.366246\n",
      "Train: [48000/95692 (50%)]\tLoss: 844.771316\n",
      "Train: [64000/95692 (67%)]\tLoss: 2196.505503\n",
      "Train: [80000/95692 (84%)]\tLoss: 1010.186217\n",
      "Epoch: 18/20. Train set: Average loss: 2213.2331\n",
      "Epoch: 18/20. Validation set: Average loss: 2020.1167\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train: [0/95692 (0%)]\tLoss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [16000/95692 (17%)]\tLoss: 1645.744733\n",
      "Train: [32000/95692 (33%)]\tLoss: 988.418991\n",
      "Train: [48000/95692 (50%)]\tLoss: 1770.718093\n",
      "Train: [64000/95692 (67%)]\tLoss: 2626.334989\n"
     ]
    }
   ],
   "source": [
    "fit(triplet_train_loader, triplet_val_loader, model, loss_fn, optimizer, scheduler, n_epochs, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
